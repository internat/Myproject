#!/usr/bin/env python3
"""
MarketPredictor AI Training - –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä—ã–Ω–∫–∞ EURUSD
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import requests
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.pipeline import Pipeline
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import json
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import tqdm

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
np.random.seed(42)
tf.random.set_seed(42)

class MarketDataProcessor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö EURUSD
    """
    
    def __init__(self, data_dir='data'):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö"""
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
    
    def fetch_market_data(self, symbol='EURUSD', interval='daily', days=365):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å Alpha Vantage API
        """
        print(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} –∑–∞ {days} –¥–Ω–µ–π...")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            cache_file = f'{self.data_dir}/{symbol}_{interval}_{days}days.csv'
            if os.path.exists(cache_file):
                print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {cache_file}")
                df = pd.read_csv(cache_file)
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                return df
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Alpha Vantage API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫–ª—é—á)
            # –ú–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫–ª—é—á –Ω–∞ https://www.alphavantage.co/support/#api-key
            api_key = "demo"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π –∫–ª—é—á
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
            if interval == 'daily':
                function = 'FX_DAILY'
                outputsize = 'full'
                interval_param = None
            else:
                function = 'FX_INTRADAY'
                outputsize = 'full'
                interval_param = interval
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –∑–∞–ø—Ä–æ—Å–∞
            url = f"https://www.alphavantage.co/query?function={function}&from_symbol=EUR&to_symbol=USD"
            if interval_param:
                url += f"&interval={interval_param}"
            url += f"&outputsize={outputsize}&apikey={api_key}"
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            print(f"üåê –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å Alpha Vantage API...")
            response = requests.get(url)
            data = response.json()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫
            if 'Error Message' in data:
                print(f"‚ùå –û—à–∏–±–∫–∞ API: {data['Error Message']}")
                return self.generate_demo_data(days, symbol)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–≤–µ—Ç–∞
            if function == 'FX_DAILY':
                time_series_key = 'Time Series FX (Daily)'
            else:
                time_series_key = f'Time Series FX ({interval_param})'
            
            if time_series_key not in data:
                print(f"‚ùå –û—à–∏–±–∫–∞: –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –æ—Ç–≤–µ—Ç–µ API")
                return self.generate_demo_data(days, symbol)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ DataFrame
            time_series = data[time_series_key]
            records = []
            
            for date, values in time_series.items():
                records.append({
                    'timestamp': date,
                    'open': float(values['1. open']),
                    'high': float(values['2. high']),
                    'low': float(values['3. low']),
                    'close': float(values['4. close']),
                    'volume': 0  # Alpha Vantage –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—ä–µ–º –¥–ª—è Forex
                })
            
            df = pd.DataFrame(records)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp')
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–Ω–µ–π
            start_date = datetime.now() - timedelta(days=days)
            df = df[df['timestamp'] >= start_date]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
            df.to_csv(cache_file, index=False)
            print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {cache_file}")
            
            return df
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            print("‚ö†Ô∏è –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
            return self.generate_demo_data(days, symbol)
    
    def generate_demo_data(self, days=365, symbol='EURUSD'):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç, –µ—Å–ª–∏ API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        """
        print(f"üìä –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} –∑–∞ {days} –¥–Ω–µ–π...")
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        base_price = 1.0850
        volatility = 0.002
        trend_strength = 0.0001
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        dates = pd.date_range(start=datetime.now() - timedelta(days=days), 
                            end=datetime.now(), freq='D')
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–µ–Ω—ã —Å —Ç—Ä–µ–Ω–¥–æ–º –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é
        np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        price_changes = np.random.normal(trend_strength, volatility, len(dates))
        prices = [base_price]
        
        for change in price_changes[1:]:
            new_price = prices[-1] + change
            prices.append(max(0.5, new_price))  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ 0.5
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame({
            'timestamp': dates,
            'open': prices,
            'high': [p + abs(np.random.normal(0, 0.0005)) for p in prices],
            'low': [p - abs(np.random.normal(0, 0.0005)) for p in prices],
            'close': [p + np.random.normal(0, 0.0005) for p in prices],
            'volume': [int(np.random.normal(1000000, 200000)) for _ in prices]
        })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ
        cache_file = f'{self.data_dir}/{symbol}_daily_{days}days_demo.csv'
        df.to_csv(cache_file, index=False)
        print(f"üíæ –î–µ–º–æ-–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {cache_file}")
        
        return df
    
    def add_technical_indicators(self, df):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∫ –¥–∞–Ω–Ω—ã–º
        """
        print("üìà –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã...")
        
        # –ö–æ–ø–∏—Ä—É–µ–º DataFrame, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
        df = df.copy()
        
        # –ü—Ä–æ—Å—Ç—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ (SMA)
        df['sma5'] = df['close'].rolling(window=5).mean()
        df['sma20'] = df['close'].rolling(window=20).mean()
        df['sma50'] = df['close'].rolling(window=50).mean()
        
        # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ (EMA)
        df['ema12'] = df['close'].ewm(span=12, adjust=False).mean()
        df['ema26'] = df['close'].ewm(span=26, adjust=False).mean()
        
        # MACD (Moving Average Convergence Divergence)
        df['macd'] = df['ema12'] - df['ema26']
        df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # RSI (Relative Strength Index)
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # Bollinger Bands
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        df['bb_std'] = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']
        df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']
        
        # –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä
        low_min = df['low'].rolling(window=14).min()
        high_max = df['high'].rolling(window=14).max()
        df['stoch_k'] = 100 * ((df['close'] - low_min) / (high_max - low_min))
        df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()
        
        # ATR (Average True Range)
        tr1 = df['high'] - df['low']
        tr2 = abs(df['high'] - df['close'].shift())
        tr3 = abs(df['low'] - df['close'].shift())
        df['tr'] = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        df['atr'] = df['tr'].rolling(window=14).mean()
        
        # Momentum
        df['momentum'] = df['close'] - df['close'].shift(10)
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        df['pct_change'] = df['close'].pct_change()
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ N –¥–Ω–µ–π
        df['target_1d'] = np.where(df['close'].shift(-1) > df['close'], 1, 0)
        df['target_3d'] = np.where(df['close'].shift(-3) > df['close'], 1, 0)
        df['target_5d'] = np.where(df['close'].shift(-5) > df['close'], 1, 0)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        df = df.dropna()
        
        return df
    
    def prepare_features_targets(self, df, target_column='target_1d', test_size=0.2):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        print("üîß –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_columns = [
            'open', 'high', 'low', 'close', 'volume',
            'sma5', 'sma20', 'sma50', 'ema12', 'ema26',
            'macd', 'macd_signal', 'macd_hist',
            'rsi', 'bb_middle', 'bb_std', 'bb_upper', 'bb_lower',
            'stoch_k', 'stoch_d', 'atr', 'momentum', 'pct_change'
        ]
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Å–∏–≤—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        X = df[feature_columns].values
        y = df[target_column].values
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, shuffle=False
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        return X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_columns

class MarketPredictor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    """
    
    def __init__(self, models_dir='models'):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π"""
        self.models_dir = models_dir
        os.makedirs(models_dir, exist_ok=True)
    
    def train_ml_model(self, X_train, y_train, model_type='random_forest'):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        print(f"üß† –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è ({model_type})...")
        
        if model_type == 'random_forest':
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏ –æ–±—É—á–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            model.fit(X_train, y_train)
            
        elif model_type == 'gradient_boosting':
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
            model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=42
            )
            model.fit(X_train, y_train)
            
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
        
        return model
    
    def train_dl_model(self, X_train, y_train, epochs=100, batch_size=32):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        print("üß† –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
        model = Sequential([
            Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
            Dropout(0.3),
            Dense(256, activation='relu'),
            Dropout(0.3),
            Dense(128, activation='relu'),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        optimizer = keras.optimizers.Adam(learning_rate=0.0005)
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–ª–±—ç–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=20,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                filepath=f"{self.models_dir}/dl_model_checkpoint.h5",
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            )
        ]
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —á–∏—Å–ª–æ–º —ç–ø–æ—Ö
        history = model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            callbacks=callbacks,
            verbose=1
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        with open(f"{self.models_dir}/model_history.json", 'w') as f:
            json.dump(history.history, f)
        
        return model, history
    
    def evaluate_model(self, model, X_test, y_test, model_type='ml'):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        """
        print("üìä –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏...")
        
        if model_type == 'dl':
            # –î–ª—è –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            y_pred_proba = model.predict(X_test)
            y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        else:
            # –î–ª—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {accuracy:.4f}")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å (Precision): {precision:.4f}")
        print(f"–ü–æ–ª–Ω–æ—Ç–∞ (Recall): {recall:.4f}")
        print(f"F1-–º–µ—Ä–∞: {f1:.4f}")
        
        # –°—Ç—Ä–æ–∏–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        plt.savefig('visualizations/confusion_matrix.png')
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
    
    def save_model(self, model, scaler, feature_columns, model_type='ml', version='v1'):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ–π –¥–∞–Ω–Ω—ã–µ
        """
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å ({model_type})...")
        
        if model_type == 'dl':
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º
            model_path = f"{self.models_dir}/dl_model_{version}.keras"
            try:
                model.save(model_path, save_format='keras')
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .keras: {e}")
                # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ h5
                model_path = f"{self.models_dir}/dl_model_{version}.h5"
                model.save(model_path, save_format='h5')
                print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path} (—Ñ–æ—Ä–º–∞—Ç h5)")
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            model_path = f"{self.models_dir}/market_predictor_model_{version}.pkl"
            joblib.dump(model, model_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä
        scaler_path = f"{self.models_dir}/scaler_{version}.pkl"
        joblib.dump(scaler, scaler_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_path = f"{self.models_dir}/feature_columns_{version}.json"
        with open(features_path, 'w') as f:
            json.dump(feature_columns, f)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
        
        return model_path
    
    def plot_feature_importance(self, model, feature_columns):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        if not hasattr(model, 'feature_importances_'):
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 8))
        plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        plt.bar(range(len(importances)), importances[indices])
        plt.xticks(range(len(importances)), [feature_columns[i] for i in indices], rotation=90)
        plt.tight_layout()
        plt.savefig('feature_importance.png')
        
        return importances, indices
    
    def plot_training_history(self, history):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        plt.figure(figsize=(12, 5))
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'])
        plt.plot(history.history['val_accuracy'])
        plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.xlabel('–≠–ø–æ—Ö–∞')
        plt.legend(['–û–±—É—á–µ–Ω–∏–µ', '–í–∞–ª–∏–¥–∞—Ü–∏—è'], loc='lower right')
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å')
        plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
        plt.xlabel('–≠–ø–æ—Ö–∞')
        plt.legend(['–û–±—É—á–µ–Ω–∏–µ', '–í–∞–ª–∏–¥–∞—Ü–∏—è'], loc='upper right')
        
        plt.tight_layout()
        plt.savefig('visualizations/dl_training_history.png')
    
    def plot_predictions(self, df, y_test, y_pred, y_pred_proba):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        test_df = df.iloc[-len(y_test):].copy()
        test_df['prediction'] = y_pred
        test_df['probability'] = y_pred_proba
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å Plotly
        fig = make_subplots(rows=2, cols=1, shared_xaxes=True, 
                           vertical_spacing=0.1, 
                           subplot_titles=('–¶–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è', '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞'))
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è
        fig.add_trace(
            go.Scatter(x=test_df['timestamp'], y=test_df['close'], 
                      name='–¶–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è', line=dict(color='blue')),
            row=1, col=1
        )
        
        # –ì—Ä–∞—Ñ–∏–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞
        fig.add_trace(
            go.Scatter(x=test_df['timestamp'], y=test_df['probability'], 
                      name='–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞', line=dict(color='green')),
            row=2, col=1
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—É—é –ª–∏–Ω–∏—é –Ω–∞ —É—Ä–æ–≤–Ω–µ 0.5
        fig.add_shape(
            type='line', line=dict(dash='dash', color='red'),
            x0=test_df['timestamp'].iloc[0], y0=0.5,
            x1=test_df['timestamp'].iloc[-1], y1=0.5,
            row=2, col=1
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–∞–∫–µ—Ç
        fig.update_layout(
            title='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏',
            height=800,
            legend=dict(orientation='h', y=1.1),
            xaxis2=dict(title='–î–∞—Ç–∞'),
            yaxis=dict(title='–¶–µ–Ω–∞ (USD)'),
            yaxis2=dict(title='–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        fig.write_html('visualizations/final_prediction.html')

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
    """
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π MarketPredictor...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
    os.makedirs('data', exist_ok=True)
    os.makedirs('visualizations', exist_ok=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä
    data_processor = MarketDataProcessor()
    predictor = MarketPredictor()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = data_processor.fetch_market_data(symbol='EURUSD', interval='daily', days=1000)
    df = data_processor.add_technical_indicators(df)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    X_train, X_test, y_train, y_test, scaler, feature_columns = data_processor.prepare_features_targets(
        df, target_column='target_1d', test_size=0.2
    )
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    ml_model = predictor.train_ml_model(X_train, y_train, model_type='random_forest')
    ml_metrics = predictor.evaluate_model(ml_model, X_test, y_test, model_type='ml')
    predictor.save_model(ml_model, scaler, feature_columns, model_type='ml', version='v1')
    
    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    predictor.plot_feature_importance(ml_model, feature_columns)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
    dl_model, history = predictor.train_dl_model(X_train, y_train, epochs=100, batch_size=32)
    dl_metrics = predictor.evaluate_model(dl_model, X_test, y_test, model_type='dl')
    predictor.save_model(dl_model, scaler, feature_columns, model_type='dl', version='v1')
    
    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
    predictor.plot_training_history(history)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictor.plot_predictions(df, y_test, dl_metrics['y_pred'], dl_metrics['y_pred_proba'])
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å ML –º–æ–¥–µ–ª–∏: {ml_metrics['accuracy']:.4f}")
    print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å DL –º–æ–¥–µ–ª–∏: {dl_metrics['accuracy']:.4f}")
    print("üìà –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ 'visualizations'")
    print("üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ 'models'")

if __name__ == "__main__":
    main()
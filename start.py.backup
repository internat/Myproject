def predict_sample(self, sample_data, use_deep_learning=True):
    """
    Makes prediction for new data using both models
    """
    if self.model is None:
        print("‚ùå Model not trained!")
        return None
    
    # Prepare data for classic model
    X = sample_data[self.feature_columns]
    
    # Make prediction with classic model
    prediction = self.model.predict(X)[0]
    probabilities = self.model.predict_proba(X)[0]
    
    result = {
        'prediction': prediction,
        'probabilities': dict(zip(self.model.classes_, probabilities)),
        'model_type': 'classic'
    }
    
    # If deep learning model exists and should be used
    if self.dl_model is not None and use_deep_learning:
        # Prepare data for deep learning
        sequence_length = 10
        
        if len(sample_data) >= sequence_length:
            # Take last sequence_length records
            sequence_data = sample_data.tail(sequence_length)[self.feature_columns].values
            
            # Normalize data
            sequence_scaled = self.scaler.transform(sequence_data)
            
            # Reshape for LSTM (add batch dimension)
            sequence_reshaped = np.array([sequence_scaled])
            
            # Make prediction
            dl_probabilities = self.dl_model.predict(sequence_reshaped)[0]
            
            # Convert indices to class labels
            class_indices = {'UP': 0, 'NEUTRAL': 1, 'DOWN': 2}
            dl_classes = {label: dl_probabilities[idx] for label, idx in class_indices.items()}
            
            # Find class with maximum probability
            dl_prediction = max(dl_classes.items(), key=lambda x: x[1])[0]
            
            # Add deep learning results
            result['dl_prediction'] = dl_prediction
            result['dl_probabilities'] = dl_classes
            
            # Combine predictions (with weights)
            ensemble_probs = {}
            for cls in self.model.classes_:
                classic_prob = result['probabilities'].get(cls, 0)
                dl_prob = dl_classes.get(cls, 0)
                # Weights: 0.6 for classic model, 0.4 for deep learning
                ensemble_probs[cls] = 0.6 * classic_prob + 0.4 * dl_prob
            
            # Find class with maximum probability
            ensemble_prediction = max(ensemble_probs.items(), key=lambda x: x[1])[0]
            
            # Add ensemble results
            result['ensemble_prediction'] = ensemble_prediction
            result['ensemble_probabilities'] = ensemble_probs
    
    return resultdef predict_sample(self, sample_data, use_deep_learning=True):
    """
    Makes prediction for new data using both models
    """
    if self.model is None:
        print("‚ùå Model not trained!")
        return None
    
    # Prepare data for classic model
    X = sample_data[self.feature_columns]
    
    # Make prediction with classic model
    prediction = self.model.predict(X)[0]
    probabilities = self.model.predict_proba(X)[0]
    
    result = {
        'prediction': prediction,
        'probabilities': dict(zip(self.model.classes_, probabilities)),
        'model_type': 'classic'
    }
    
    # If deep learning model exists and should be used
    if self.dl_model is not None and use_deep_learning:
        # Prepare data for deep learning
        sequence_length = 10
        
        if len(sample_data) >= sequence_length:
            # Take last sequence_length records
            sequence_data = sample_data.tail(sequence_length)[self.feature_columns].values
            
            # Normalize data
            sequence_scaled = self.scaler.transform(sequence_data)
            
            # Reshape for LSTM (add batch dimension)
            sequence_reshaped = np.array([sequence_scaled])
            
            # Make prediction
            dl_probabilities = self.dl_model.predict(sequence_reshaped)[0]
            
            # Convert indices to class labels
            class_indices = {'UP': 0, 'NEUTRAL': 1, 'DOWN': 2}
            dl_classes = {label: dl_probabilities[idx] for label, idx in class_indices.items()}
            
            # Find class with maximum probability
            dl_prediction = max(dl_classes.items(), key=lambda x: x[1])[0]
            
            # Add deep learning results
            result['dl_prediction'] = dl_prediction
            result['dl_probabilities'] = dl_classes
            
            # Combine predictions (with weights)
            ensemble_probs = {}
            for cls in self.model.classes_:
                classic_prob = result['probabilities'].get(cls, 0)
                dl_prob = dl_classes.get(cls, 0)
                # Weights: 0.6 for classic model, 0.4 for deep learning
                ensemble_probs[cls] = 0.6 * classic_prob + 0.4 * dl_prob
            
            # Find class with maximum probability
            ensemble_prediction = max(ensemble_probs.items(), key=lambda x: x[1])[0]
            
            # Add ensemble results
            result['ensemble_prediction'] = ensemble_prediction
            result['ensemble_probabilities'] = ensemble_probs
    
    return resultdef predict_sample(self, sample_data, use_deep_learning=True):
    """
    Makes prediction for new data using both models
    """
    if self.model is None:
        print("‚ùå Model not trained!")
        return None
    
    # Prepare data for classic model
    X = sample_data[self.feature_columns]
    
    # Make prediction with classic model
    prediction = self.model.predict(X)[0]
    probabilities = self.model.predict_proba(X)[0]
    
    result = {
        'prediction': prediction,
        'probabilities': dict(zip(self.model.classes_, probabilities)),
        'model_type': 'classic'
    }
    
    # If deep learning model exists and should be used
    if self.dl_model is not None and use_deep_learning:
        # Prepare data for deep learning
        sequence_length = 10
        
        if len(sample_data) >= sequence_length:
            # Take last sequence_length records
            sequence_data = sample_data.tail(sequence_length)[self.feature_columns].values
            
            # Normalize data
            sequence_scaled = self.scaler.transform(sequence_data)
            
            # Reshape for LSTM (add batch dimension)
            sequence_reshaped = np.array([sequence_scaled])
            
            # Make prediction
            dl_probabilities = self.dl_model.predict(sequence_reshaped)[0]
            
            # Convert indices to class labels
            class_indices = {'UP': 0, 'NEUTRAL': 1, 'DOWN': 2}
            dl_classes = {label: dl_probabilities[idx] for label, idx in class_indices.items()}
            
            # Find class with maximum probability
            dl_prediction = max(dl_classes.items(), key=lambda x: x[1])[0]
            
            # Add deep learning results
            result['dl_prediction'] = dl_prediction
            result['dl_probabilities'] = dl_classes
            
            # Combine predictions (with weights)
            ensemble_probs = {}
            for cls in self.model.classes_:
                classic_prob = result['probabilities'].get(cls, 0)
                dl_prob = dl_classes.get(cls, 0)
                # Weights: 0.6 for classic model, 0.4 for deep learning
                ensemble_probs[cls] = 0.6 * classic_prob + 0.4 * dl_prob
            
            # Find class with maximum probability
            ensemble_prediction = max(ensemble_probs.items(), key=lambda x: x[1])[0]
            
            # Add ensemble results
            result['ensemble_prediction'] = ensemble_prediction
            result['ensemble_probabilities'] = ensemble_probs
    
    return resultasync analyzeTimeframe(marketData, timeframe) {
    // Simulation of technical analysis for specific timeframe
    const rsi = 30 + Math.random() * 40; // RSI from 30 to 70
    const macd = (Math.random() - 0.5) * 0.002;
    const sma20 = parseFloat(marketData.currentPrice) + (Math.random() - 0.5) * 0.005;
    const ema12 = parseFloat(marketData.currentPrice) + (Math.random() - 0.5) * 0.003;
    
    // Determine direction based on technical indicators
    let direction = 'NEUTRAL';
    let confidence = 50;
    let reason = '';

    // SIGNAL GENERATION CONDITIONS HERE:
    if (rsi < 30 && macd > 0) {
        direction = 'UP';
        confidence = 70 + Math.random() * 20;
        reason = 'Oversold + positive MACD';
    } else if (rsi > 70 && macd < 0) {
        direction = 'DOWN';
        confidence = 70 + Math.random() * 20;
        reason = 'Overbought + negative MACD';
    } else if (parseFloat(marketData.currentPrice) > sma20 && ema12 > sma20) {
        // EMA > SMA condition here
        direction = 'UP';
        confidence = 60 + Math.random() * 15;
        reason = 'Price above SMA20 and EMA12';
    } else if (parseFloat(marketData.currentPrice) < sma20 && ema12 < sma20) {
        direction = 'DOWN';
        confidence = 60 + Math.random() * 15;
        reason = 'Price below SMA20 and EMA12';
    }

    return {
        direction,
        confidence: Math.round(confidence),
        reason,
        indicators: { rsi: Math.round(rsi), macd, sma20, ema12 },
        timeframe: timeframe.name
    };
}#!/usr/bin/env python3
"""
MarketPredictor - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ò–ò –º–æ–¥–µ–ª—å —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ä—ã–Ω–∫–∞
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import joblib
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from datetime import datetime, timedelta
import os
import json
import requests
import warnings
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
warnings.filterwarnings('ignore')

class MarketPredictorAI:
    def __init__(self, use_deep_learning=True, use_advanced_analysis=True):
        self.model = None
        self.dl_model = None
        self.feature_columns = []
        self.target_column = 'direction'
        self.scaler = StandardScaler()
        self.use_deep_learning = use_deep_learning
        self.use_advanced_analysis = use_advanced_analysis
        self.model_history = []
        self.performance_history = []
        self.error_analysis = {}
        self.last_update = datetime.now()
        self.auto_update_interval = 24  # —á–∞—Å—ã
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö
        os.makedirs('models', exist_ok=True)
        os.makedirs('data', exist_ok=True)
        os.makedirs('visualizations', exist_ok=True)
        
    def fetch_market_data(self, symbol='EURUSD', interval='5min', days=365):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å Alpha Vantage API
        """
        print(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} –∑–∞ {days} –¥–Ω–µ–π...")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            cache_file = f'data/{symbol}_{interval}_{days}days.csv'
            if os.path.exists(cache_file):
                print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {cache_file}")
                df = pd.read_csv(cache_file)
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                return df
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Alpha Vantage API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫–ª—é—á)
            # –ú–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫–ª—é—á –Ω–∞ https://www.alphavantage.co/support/#api-key
            api_key = "demo"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π –∫–ª—é—á
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
            if interval == '5min':
                function = 'TIME_SERIES_INTRADAY'
                outputsize = 'full'
                interval_param = '5min'
            elif interval == 'daily':
                function = 'TIME_SERIES_DAILY'
                outputsize = 'full'
                interval_param = None
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: {interval}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –∑–∞–ø—Ä–æ—Å–∞
            base_url = "https://www.alphavantage.co/query"
            params = {
                "function": function,
                "symbol": symbol,
                "apikey": api_key,
                "outputsize": outputsize
            }
            
            if interval_param:
                params["interval"] = interval_param
            
            print(f"üåê –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Alpha Vantage API...")
            response = requests.get(base_url, params=params)
            
            if response.status_code != 200:
                raise Exception(f"–û—à–∏–±–∫–∞ API: {response.status_code}")
            
            data = response.json()
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
            if function == 'TIME_SERIES_INTRADAY':
                time_series_key = f'Time Series ({interval_param})'
            else:
                time_series_key = 'Time Series (Daily)'
            
            if time_series_key not in data:
                # –ï—Å–ª–∏ API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
                print("‚ö†Ô∏è API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ.")
                return self.generate_demo_data(days, symbol)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ DataFrame
            time_series = data[time_series_key]
            records = []
            
            for timestamp, values in time_series.items():
                record = {
                    'timestamp': timestamp,
                    'open': float(values['1. open']),
                    'high': float(values['2. high']),
                    'low': float(values['3. low']),
                    'close': float(values['4. close']),
                    'volume': int(values['5. volume'])
                }
                records.append(record)
            
            df = pd.DataFrame(records)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp')
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–Ω–µ–π
            start_date = datetime.now() - timedelta(days=days)
            df = df[df['timestamp'] >= start_date]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
            df.to_csv(cache_file, index=False)
            print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {cache_file}")
            
            return df
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
            print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ –≤–º–µ—Å—Ç–æ —Ä–µ–∞–ª—å–Ω—ã—Ö.")
            return self.generate_demo_data(days, symbol)
    
    def generate_demo_data(self, days=365, symbol='EURUSD'):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç, –µ—Å–ª–∏ API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        """
        print(f"üìä –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} –∑–∞ {days} –¥–Ω–µ–π...")
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        base_price = 1.0850
        volatility = 0.002
        trend_strength = 0.0001
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        dates = pd.date_range(start=datetime.now() - timedelta(days=days), 
                            end=datetime.now(), freq='5min')
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–µ–Ω—ã —Å —Ç—Ä–µ–Ω–¥–æ–º –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é
        np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        price_changes = np.random.normal(trend_strength, volatility, len(dates))
        prices = [base_price]
        
        for change in price_changes[1:]:
            new_price = prices[-1] + change
            prices.append(max(0.5, new_price))  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ 0.5
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame({
            'timestamp': dates,
            'open': prices,
            'high': [p + abs(np.random.normal(0, 0.0005)) for p in prices],
            'low': [p - abs(np.random.normal(0, 0.0005)) for p in prices],
            'close': prices,
            'volume': np.random.randint(100000, 1000000, len(dates))
        })
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º high/low
        df['high'] = df[['open', 'high', 'close']].max(axis=1)
        df['low'] = df[['open', 'low', 'close']].min(axis=1)
        
        return df
    
    def calculate_technical_indicators(self, df):
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        """
        print("üîß –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã...")
        
        # RSI (Relative Strength Index)
        def calculate_rsi(prices, period=14):
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi
        
        # MACD (Moving Average Convergence Divergence)
        def calculate_macd(prices, fast=12, slow=26, signal=9):
            ema_fast = prices.ewm(span=fast).mean()
            ema_slow = prices.ewm(span=slow).mean()
            macd_line = ema_fast - ema_slow
            signal_line = macd_line.ewm(span=signal).mean()
            histogram = macd_line - signal_line
            return macd_line, signal_line, histogram
        
        # Bollinger Bands
        def calculate_bollinger_bands(prices, period=20, std_dev=2):
            sma = prices.rolling(window=period).mean()
            std = prices.rolling(window=period).std()
            upper_band = sma + (std * std_dev)
            lower_band = sma - (std * std_dev)
            return upper_band, sma, lower_band
        
        # Stochastic Oscillator
        def calculate_stochastic(high, low, close, period=14):
            lowest_low = low.rolling(window=period).min()
            highest_high = high.rolling(window=period).max()
            k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))
            d_percent = k_percent.rolling(window=3).mean()
            return k_percent, d_percent
        
        # Williams %R
        def calculate_williams_r(high, low, close, period=14):
            highest_high = high.rolling(window=period).max()
            lowest_low = low.rolling(window=period).min()
            williams_r = -100 * ((highest_high - close) / (highest_high - lowest_low))
            return williams_r
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['rsi'] = calculate_rsi(df['close'])
        df['macd'], df['macd_signal'], df['macd_histogram'] = calculate_macd(df['close'])
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        df['sma_20'] = df['close'].rolling(window=20).mean()
        df['sma_50'] = df['close'].rolling(window=50).mean()
        df['ema_12'] = df['close'].ewm(span=12).mean()
        df['ema_26'] = df['close'].ewm(span=26).mean()
        
        # Bollinger Bands
        df['bb_upper'], df['bb_middle'], df['bb_lower'] = calculate_bollinger_bands(df['close'])
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # Stochastic
        df['stoch_k'], df['stoch_d'] = calculate_stochastic(df['high'], df['low'], df['close'])
        
        # Williams %R
        df['williams_r'] = calculate_williams_r(df['high'], df['low'], df['close'])
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['price_change'] = df['close'].pct_change()
        df['volume_sma'] = df['volume'].rolling(window=20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        
        # ATR (Average True Range)
        df['tr1'] = df['high'] - df['low']
        df['tr2'] = abs(df['high'] - df['close'].shift())
        df['tr3'] = abs(df['low'] - df['close'].shift())
        df['true_range'] = df[['tr1', 'tr2', 'tr3']].max(axis=1)
        df['atr'] = df['true_range'].rolling(window=14).mean()
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        df = df.drop(['tr1', 'tr2', 'tr3'], axis=1)
        
        return df
    
    def create_target_variable(self, df, future_periods=12):
        """
        –°–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è)
        """
        print("üéØ –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é...")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –±—É–¥—É—â—É—é —Ü–µ–Ω—É
        df['future_price'] = df['close'].shift(-future_periods)
        df['price_change_future'] = (df['future_price'] - df['close']) / df['close']
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        df['direction'] = 'NEUTRAL'
        df.loc[df['price_change_future'] > 0.001, 'direction'] = 'UP'      # –†–æ—Å—Ç > 0.1%
        df.loc[df['price_change_future'] < -0.001, 'direction'] = 'DOWN'   # –ü–∞–¥–µ–Ω–∏–µ > 0.1%
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
        df = df.dropna()
        
        return df
    
    def prepare_features(self, df):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        print("üîç –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏...")
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_columns = [
            'rsi', 'macd', 'macd_signal', 'macd_histogram',
            'sma_20', 'sma_50', 'ema_12', 'ema_26',
            'bb_width', 'bb_position', 'stoch_k', 'stoch_d',
            'williams_r', 'price_change', 'volume_ratio', 'atr'
        ]
        
        # –°–æ–∑–¥–∞–µ–º –ª–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for col in ['close', 'volume', 'rsi', 'macd']:
            for lag in [1, 2, 3, 5]:
                df[f'{col}_lag_{lag}'] = df[col].shift(lag)
                feature_columns.append(f'{col}_lag_{lag}')
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç—Ä–µ–Ω–¥–∞
        df['trend_5'] = (df['close'] - df['close'].shift(5)) / df['close'].shift(5)
        df['trend_10'] = (df['close'] - df['close'].shift(10)) / df['close'].shift(10)
        df['trend_20'] = (df['close'] - df['close'].shift(20)) / df['close'].shift(20)
        
        feature_columns.extend(['trend_5', 'trend_10', 'trend_20'])
        
        # –£–¥–∞–ª—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        df = df.dropna()
        
        self.feature_columns = feature_columns
        
        return df
    
    def train_model(self, df):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        print("ü§ñ –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å...")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X = df[self.feature_columns]
        y = df[self.target_column]
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {accuracy:.2%}")
        print("\nüìä –û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(y_test, y_pred))
        
        return accuracy
    
    def analyze_feature_importance(self, df):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        print("üìà –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("\nüèÜ –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        print(feature_importance.head(10))
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 8))
        sns.barplot(data=feature_importance.head(15), x='importance', y='feature')
        plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
        plt.tight_layout()
        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return feature_importance
    
    def save_model(self, filename='models/market_predictor_model.pkl', version=None):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        if version is None:
            version = len(self.model_history) + 1
            
        model_filename = filename.replace('.pkl', f'_v{version}.pkl')
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ {model_filename}...")
        
        model_data = {
            'model': self.model,
            'dl_model_path': f'models/dl_model_v{version}.h5' if self.dl_model else None,
            'feature_columns': self.feature_columns,
            'target_column': self.target_column,
            'scaler': self.scaler,
            'training_date': datetime.now().isoformat(),
            'version': version,
            'performance': self.performance_history[-1] if self.performance_history else None,
            'error_analysis': self.error_analysis
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
        joblib.dump(model_data, model_filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if self.dl_model:
            self.dl_model.save(f'models/dl_model_v{version}.h5')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.model_history.append({
            'version': version,
            'filename': model_filename,
            'training_date': datetime.now().isoformat(),
            'performance': self.performance_history[-1] if self.performance_history else None
        })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –º–æ–¥–µ–ª–µ–π
        with open('models/model_history.json', 'w') as f:
            json.dump(self.model_history, f, indent=4)
            
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
        
    def load_best_model(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        """
        if not os.path.exists('models/model_history.json'):
            print("‚ùå –ò—Å—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return False
            
        with open('models/model_history.json', 'r') as f:
            history = json.load(f)
            
        if not history:
            print("‚ùå –ò—Å—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –ø—É—Å—Ç–∞!")
            return False
            
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏
        best_model = max(history, key=lambda x: x['performance']['accuracy'] if x['performance'] else 0)
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (–≤–µ—Ä—Å–∏—è {best_model['version']})...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model_data = joblib.load(best_model['filename'])
        self.model = model_data['model']
        self.feature_columns = model_data['feature_columns']
        self.target_column = model_data['target_column']
        self.scaler = model_data.get('scaler', StandardScaler())
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if model_data.get('dl_model_path') and os.path.exists(model_data['dl_model_path']):
            self.dl_model = load_model(model_data['dl_model_path'])
            
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –¢–æ—á–Ω–æ—Å—Ç—å: {model_data['performance']['accuracy']:.2%}")
        return True
        
    def analyze_errors(self, X_test, y_test, y_pred):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è
        """
        print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏...")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        results_df = pd.DataFrame({
            'actual': y_test,
            'predicted': y_pred
        })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        for col in X_test.columns:
            results_df[col] = X_test[col].values
            
        # –ù–∞—Ö–æ–¥–∏–º –æ—à–∏–±–∫–∏
        results_df['is_error'] = results_df['actual'] != results_df['predicted']
        errors_df = results_df[results_df['is_error']]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–æ —Ç–∏–ø–∞–º
        error_types = {}
        for actual in results_df['actual'].unique():
            for predicted in results_df['predicted'].unique():
                if actual != predicted:
                    count = len(results_df[(results_df['actual'] == actual) & (results_df['predicted'] == predicted)])
                    if count > 0:
                        error_types[f'{actual}->{predicted}'] = count
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –æ—à–∏–±–∫–∞–º–∏
        feature_error_correlation = {}
        for col in X_test.columns:
            if results_df[col].std() > 0:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–æ–º –∏ –æ—à–∏–±–∫–æ–π
                correlation = np.corrcoef(results_df[col], results_df['is_error'])[0, 1]
                feature_error_correlation[col] = correlation
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        sorted_features = sorted(feature_error_correlation.items(), key=lambda x: abs(x[1]), reverse=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.error_analysis = {
            'error_rate': len(errors_df) / len(results_df),
            'error_types': error_types,
            'feature_correlation': dict(sorted_features[:10]),
            'error_examples': errors_df.head(10).to_dict(orient='records')
        }
        
        print(f"üìä –ß–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫: {self.error_analysis['error_rate']:.2%}")
        print("üîç –¢–∏–ø—ã –æ—à–∏–±–æ–∫:")
        for error_type, count in error_types.items():
            print(f"  {error_type}: {count} —Å–ª—É—á–∞–µ–≤")
            
        print("üîç –¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –æ—à–∏–±–∫–∞–º–∏:")
        for feature, corr in sorted_features[:5]:
            print(f"  {feature}: {corr:.4f}")
            
        return self.error_analysis
        
    def create_deep_learning_model(self, input_shape):
        """
        –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        """
        model = Sequential()
        
        # –°–ª–æ–π LSTM –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))
        model.add(Dropout(0.2))
        model.add(BatchNormalization())
        
        # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π LSTM
        model.add(LSTM(units=50, return_sequences=False))
        model.add(Dropout(0.2))
        model.add(BatchNormalization())
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        model.add(Dense(units=32, activation='relu'))
        model.add(Dropout(0.2))
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–¥–ª—è —Ç—Ä–µ—Ö –∫–ª–∞—Å—Å–æ–≤: UP, DOWN, NEUTRAL)
        model.add(Dense(units=3, activation='softmax'))
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
        
    def prepare_data_for_deep_learning(self, df, sequence_length=10):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        target_map = {'UP': 0, 'NEUTRAL': 1, 'DOWN': 2}
        y_numeric = df[self.target_column].map(target_map).values
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X = df[self.feature_columns].values
        X_scaled = self.scaler.fit_transform(X)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è LSTM
        X_sequences = []
        y_sequences = []
        
        for i in range(len(X_scaled) - sequence_length):
            X_sequences.append(X_scaled[i:i+sequence_length])
            y_sequences.append(y_numeric[i+sequence_length])
            
        return np.array(X_sequences), np.array(y_sequences)
        
    def train_deep_learning_model(self, df, epochs=50, batch_size=32, sequence_length=10):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        print("üß† –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X_sequences, y_sequences = self.prepare_data_for_deep_learning(df, sequence_length)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X_train, X_test, y_train, y_test = train_test_split(
            X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=y_sequences
        )
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.dl_model = self.create_deep_learning_model((sequence_length, X_train.shape[2]))
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–ª–±—ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001),
            ModelCheckpoint('models/dl_model_checkpoint.h5', save_best_only=True)
        ]
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        history = self.dl_model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            callbacks=callbacks,
            verbose=1
        )
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        evaluation = self.dl_model.evaluate(X_test, y_test)
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {evaluation[1]:.2%}")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
        plt.figure(figsize=(12, 5))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'], label='–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞')
        plt.plot(history.history['val_loss'], label='–ü—Ä–æ–≤–µ—Ä–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞')
        plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å')
        plt.xlabel('–≠–ø–æ—Ö–∞')
        plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
        plt.legend()
        
        plt.subplot(1, 2, 2)
        plt.plot(history.history['accuracy'], label='–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞')
        plt.plot(history.history['val_accuracy'], label='–ü—Ä–æ–≤–µ—Ä–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞')
        plt.title('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.xlabel('–≠–ø–æ—Ö–∞')
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('visualizations/dl_training_history.png', dpi=300, bbox_inches='tight')
        
        return evaluation[1]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
        
    def check_and_update_data(self):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        now = datetime.now()
        
        # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –∏–ª–∏ –ø—Ä–æ—à–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        if self.last_update is None or (now - self.last_update).total_seconds() > self.auto_update_interval * 3600:
            print(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            data = self.fetch_market_data()
            
            # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
            self.train_model(data)
            if self.use_deep_learning:
                self.train_deep_learning_model(data)
                
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            self.last_update = now
            print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {now.strftime('%Y-%m-%d %H:%M:%S')}")
            
            return True
        
        return False
            
    def perform_advanced_analysis(self, market_data, question=None):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
        """
        if not self.use_advanced_analysis:
            return "–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª—é—á–µ–Ω."
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.check_and_update_data()
            
        print("üß† –í—ã–ø–æ–ª–Ω—è–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        recent_data = market_data.tail(30).copy()
        
        try:
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –ø–µ—Ä–∏–æ–¥–æ–≤)
            volatility = recent_data['close'].rolling(window=20).std().iloc[-1]
            
            # –¢—Ä–µ–Ω–¥ (–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö —Å—Ä–µ–¥–Ω–∏—Ö)
            recent_data['sma_20'] = recent_data['close'].rolling(window=20).mean()
            recent_data['sma_50'] = recent_data['close'].rolling(window=50).mean()
            recent_data['sma_200'] = recent_data['close'].rolling(window=200).mean()
            
            sma_20 = recent_data['sma_20'].iloc[-1]
            sma_50 = recent_data['sma_50'].iloc[-1]
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è SMA 200
            if pd.notna(recent_data['sma_200'].iloc[-1]):
                sma_200 = recent_data['sma_200'].iloc[-1]
            else:
                sma_200 = recent_data['close'].mean()
            
            # –¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
            current_price = recent_data['close'].iloc[-1]
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º EMA –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤
            recent_data['ema_12'] = recent_data['close'].ewm(span=12).mean()
            recent_data['ema_26'] = recent_data['close'].ewm(span=26).mean()
            
            ema_12 = recent_data['ema_12'].iloc[-1]
            ema_26 = recent_data['ema_26'].iloc[-1]
            
            # SIGNAL GENERATION: EMA > SMA CONDITIONS
            signal_strength = 0
            signal_direction = "NEUTRAL"
            probability = 50
            
            # EMA > SMA crossover signals with probability conditions
            if ema_12 > sma_20 and sma_20 > sma_50 and current_price > ema_12:
                signal_direction = "BUY"
                probability = 75 + min(20, (ema_12 - sma_20) / sma_20 * 1000)  # Scale probability
                signal_strength = 3
                signal_reason = "EMA12 > SMA20 > SMA50 + Price above EMA12"
            elif ema_12 < sma_20 and sma_20 < sma_50 and current_price < ema_12:
                signal_direction = "SELL"
                probability = 75 + min(20, (sma_20 - ema_12) / sma_20 * 1000)  # Scale probability
                signal_strength = 3
                signal_reason = "EMA12 < SMA20 < SMA50 + Price below EMA12"
            elif ema_12 > ema_26 and probability > 60:
                signal_direction = "BUY"
                probability = min(85, 60 + (ema_12 - ema_26) / ema_26 * 1000)
                signal_strength = 2
                signal_reason = "EMA12 > EMA26 crossover"
            elif ema_12 < ema_26 and probability > 60:
                signal_direction = "SELL"
                probability = min(85, 60 + (ema_26 - ema_12) / ema_26 * 1000)
                signal_strength = 2
                signal_reason = "EMA12 < EMA26 crossover"
            
            # Ensure probability is within valid range (50-95%)
            probability = max(50, min(95, probability))
            
            print(f"üéØ SIGNAL GENERATED: {signal_direction} | Strength: {signal_strength} | Probability: {probability:.1f}% | Reason: {signal_reason if 'signal_reason' in locals() else 'No clear signal'}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–ª—É —Ç—Ä–µ–Ω–¥–∞
            if sma_20 > sma_50 and sma_50 > sma_200:
                trend = "–°–∏–ª—å–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π"
            elif sma_20 > sma_50:
                trend = "–£–º–µ—Ä–µ–Ω–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π"
            elif sma_20 < sma_50 and sma_50 < sma_200:
                trend = "–°–∏–ª—å–Ω—ã–π –Ω–∏—Å—Ö–æ–¥—è—â–∏–π"
            elif sma_20 < sma_50:
                trend = "–£–º–µ—Ä–µ–Ω–Ω—ã–π –Ω–∏—Å—Ö–æ–¥—è—â–∏–π"
            else:
                trend = "–ë–æ–∫–æ–≤–æ–π"
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
            support_level = recent_data['low'].rolling(window=20).min().iloc[-1]
            resistance_level = recent_data['high'].rolling(window=20).max().iloc[-1]
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º RSI
            delta = recent_data['close'].diff()
            gain = delta.where(delta > 0, 0).rolling(window=14).mean()
            loss = -delta.where(delta < 0, 0).rolling(window=14).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs)).iloc[-1]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å/–ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å
            if rsi > 70:
                rsi_signal = "–ü–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å"
            elif rsi < 30:
                rsi_signal = "–ü–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å"
            else:
                rsi_signal = "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞
            market_analysis = f"""
            # –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏
            
            ## –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞
            - –¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞: {current_price:.4f}
            - –¢—Ä–µ–Ω–¥: {trend}
            - –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {volatility:.4f}
            - RSI (14): {rsi:.2f} - {rsi_signal}
            
            ## –ö–ª—é—á–µ–≤—ã–µ —É—Ä–æ–≤–Ω–∏
            - –£—Ä–æ–≤–µ–Ω—å –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {support_level:.4f}
            - –£—Ä–æ–≤–µ–Ω—å —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è: {resistance_level:.4f}
            - –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {(current_price - support_level) / current_price * 100:.2f}%
            - –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è: {(resistance_level - current_price) / current_price * 100:.2f}%
            
            ## –í–æ–∑–º–æ–∂–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
            """
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏
            if trend.startswith("–°–∏–ª—å–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π"):
                market_analysis += """
                1. –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤–æ—Å—Ö–æ–¥—è—â–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞ —Å —Ü–µ–ª—å—é –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
                2. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –∫ —É—Ä–æ–≤–Ω—é SMA 20 —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º —Ä–æ—Å—Ç–∞
                3. –ü—Ä–æ–±–æ–π —É—Ä–æ–≤–Ω—è —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–æ—Å—Ö–æ–¥—è—â–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
                """
            elif trend.startswith("–£–º–µ—Ä–µ–Ω–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π"):
                market_analysis += """
                1. –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤–æ—Å—Ö–æ–¥—è—â–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è —Å –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ–º —Ç–µ–º–ø–∞ —Ä–æ—Å—Ç–∞
                2. –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –º–µ–∂–¥—É —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω–æ–π –∏ —É—Ä–æ–≤–Ω–µ–º —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
                3. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –∫ —É—Ä–æ–≤–Ω—é –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –æ—Ç—Å–∫–æ–∫–æ–º
                """
            elif trend.startswith("–°–∏–ª—å–Ω—ã–π –Ω–∏—Å—Ö–æ–¥—è—â–∏–π"):
                market_analysis += """
                1. –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –Ω–∏—Å—Ö–æ–¥—è—â–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞ —Å —Ü–µ–ª—å—é –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∫–∏
                2. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –∫ —É—Ä–æ–≤–Ω—é SMA 20 —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º –ø–∞–¥–µ–Ω–∏—è
                3. –ü—Ä–æ–±–æ–π —É—Ä–æ–≤–Ω—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∏—Å—Ö–æ–¥—è—â–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
                """
            elif trend.startswith("–£–º–µ—Ä–µ–Ω–Ω—ã–π –Ω–∏—Å—Ö–æ–¥—è—â–∏–π"):
                market_analysis += """
                1. –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –Ω–∏—Å—Ö–æ–¥—è—â–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è —Å –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ–º —Ç–µ–º–ø–∞ –ø–∞–¥–µ–Ω–∏—è
                2. –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –º–µ–∂–¥—É —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω–æ–π –∏ —É—Ä–æ–≤–Ω–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫–∏
                3. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –∫ —É—Ä–æ–≤–Ω—é —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º —Å–Ω–∏–∂–µ–Ω–∏–µ–º
                """
            else:
                market_analysis += """
                1. –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –±–æ–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è –≤ —Ç–µ–∫—É—â–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
                2. –ü—Ä–æ–±–æ–π —É—Ä–æ–≤–Ω—è —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è –∏ –Ω–∞—á–∞–ª–æ –≤–æ—Å—Ö–æ–¥—è—â–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞
                3. –ü—Ä–æ–±–æ–π —É—Ä–æ–≤–Ω—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ –Ω–∞—á–∞–ª–æ –Ω–∏—Å—Ö–æ–¥—è—â–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞
                """
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—Ä–≥–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
            trading_strategy = f"""
            # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Ç–æ—Ä–≥–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
            
            ## –¢–æ—á–∫–∏ –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞
            """
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏—Ç—É–∞—Ü–∏–∏
            if trend.startswith("–°–∏–ª—å–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π") or trend.startswith("–£–º–µ—Ä–µ–Ω–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π"):
                if rsi_signal == "–ü–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å":
                    trading_strategy += """
                    - –í—Ö–æ–¥: –î–æ–∂–¥–∞—Ç—å—Å—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∫ SMA 20 –∏ –æ—Ç–∫—Ä—ã—Ç—å –¥–ª–∏–Ω–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                    - –í—ã—Ö–æ–¥: –ü—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —É—Ä–æ–≤–Ω—è —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è –∏–ª–∏ –ø—Ä–∏ —Ä–∞–∑–≤–æ—Ä–æ—Ç–µ —Ü–µ–Ω—ã
                    - –°—Ç–æ–ø-–ª–æ—Å—Å: –ù–∏–∂–µ —É—Ä–æ–≤–Ω—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –Ω–∞ 0.5%
                    """
                else:
                    trading_strategy += """
                    - –í—Ö–æ–¥: –û—Ç–∫—Ä—ã—Ç—å –¥–ª–∏–Ω–Ω—É—é –ø–æ–∑–∏—Ü–∏—é –Ω–∞ —Ç–µ–∫—É—â–∏—Ö —É—Ä–æ–≤–Ω—è—Ö
                    - –í—ã—Ö–æ–¥: –ü—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —É—Ä–æ–≤–Ω—è —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è –∏–ª–∏ –ø—Ä–∏ —Ä–∞–∑–≤–æ—Ä–æ—Ç–µ —Ü–µ–Ω—ã
                    - –°—Ç–æ–ø-–ª–æ—Å—Å: –ù–∏–∂–µ SMA 50 –Ω–∞ 0.5%
                    """
            elif trend.startswith("–°–∏–ª—å–Ω—ã–π –Ω–∏—Å—Ö–æ–¥—è—â–∏–π") or trend.startswith("–£–º–µ—Ä–µ–Ω–Ω—ã–π –Ω–∏—Å—Ö–æ–¥—è—â–∏–π"):
                if rsi_signal == "–ü–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å":
                    trading_strategy += """
                    - –í—Ö–æ–¥: –î–æ–∂–¥–∞—Ç—å—Å—è –æ—Ç—Å–∫–æ–∫–∞ –∫ SMA 20 –∏ –æ—Ç–∫—Ä—ã—Ç—å –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–∑–∏—Ü–∏—é
                    - –í—ã—Ö–æ–¥: –ü—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —É—Ä–æ–≤–Ω—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏–ª–∏ –ø—Ä–∏ —Ä–∞–∑–≤–æ—Ä–æ—Ç–µ —Ü–µ–Ω—ã
                    - –°—Ç–æ–ø-–ª–æ—Å—Å: –í—ã—à–µ —É—Ä–æ–≤–Ω—è —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è –Ω–∞ 0.5%
                    """
                else:
                    trading_strategy += """
                    - –í—Ö–æ–¥: –û—Ç–∫—Ä—ã—Ç—å –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–∑–∏—Ü–∏—é –Ω–∞ —Ç–µ–∫—É—â–∏—Ö —É—Ä–æ–≤–Ω—è—Ö
                    - –í—ã—Ö–æ–¥: –ü—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —É—Ä–æ–≤–Ω—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏–ª–∏ –ø—Ä–∏ —Ä–∞–∑–≤–æ—Ä–æ—Ç–µ —Ü–µ–Ω—ã
                    - –°—Ç–æ–ø-–ª–æ—Å—Å: –í—ã—à–µ SMA 50 –Ω–∞ 0.5%
                    """
            else:
                trading_strategy += """
                - –í—Ö–æ–¥: –í–æ–∑–¥–µ—Ä–∂–∞—Ç—å—Å—è –æ—Ç –≤—Ö–æ–¥–∞ –≤ —Ä—ã–Ω–æ–∫ –¥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ç–∫–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞
                - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ: –¢–æ—Ä–≥–æ–≤–∞—Ç—å –æ—Ç–±–æ–∏ –æ—Ç –≥—Ä–∞–Ω–∏—Ü –¥–∏–∞–ø–∞–∑–æ–Ω–∞
                - –°—Ç–æ–ø-–ª–æ—Å—Å: –ó–∞ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–π –≥—Ä–∞–Ω–∏—Ü–µ–π –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –Ω–∞ 0.5%
                """
            
            trading_strategy += f"""
            ## –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏
            - –†–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏: –ù–µ –±–æ–ª–µ–µ 2% –æ—Ç –∫–∞–ø–∏—Ç–∞–ª–∞
            - –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ä–∏—Å–∫/–ø—Ä–∏–±—ã–ª—å: –ú–∏–Ω–∏–º—É–º 1:2
            - –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Ä—ã–Ω–∫–∞: {volatility:.4f} - {"–í—ã—Å–æ–∫–∞—è" if volatility > 0.01 else "–°—Ä–µ–¥–Ω—è—è" if volatility > 0.005 else "–ù–∏–∑–∫–∞—è"}
            
            ## –í—Ä–µ–º–µ–Ω–Ω–æ–π –≥–æ—Ä–∏–∑–æ–Ω—Ç
            - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≥–æ—Ä–∏–∑–æ–Ω—Ç: {"–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π (1-3 –¥–Ω—è)" if volatility > 0.01 else "–°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–π (1-2 –Ω–µ–¥–µ–ª–∏)" if volatility > 0.005 else "–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π (2+ –Ω–µ–¥–µ–ª–∏)"}
            """
            
            # –ï—Å–ª–∏ –±—ã–ª –∑–∞–¥–∞–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –Ω–µ–≥–æ
            if question:
                market_analysis += f"\n\n## –û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}\n"
                
                if "—Ç—Ä–µ–Ω–¥" in question.lower():
                    market_analysis += f"–¢–µ–∫—É—â–∏–π —Ç—Ä–µ–Ω–¥: {trend}. "
                    
                if "–≤—Ö–æ–¥" in question.lower() or "–ø–æ–∑–∏—Ü" in question.lower():
                    if trend.startswith("–°–∏–ª—å–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π") or trend.startswith("–£–º–µ—Ä–µ–Ω–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π"):
                        market_analysis += "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç–∏–µ –¥–ª–∏–Ω–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏. "
                    elif trend.startswith("–°–∏–ª—å–Ω—ã–π –Ω–∏—Å—Ö–æ–¥—è—â–∏–π") or trend.startswith("–£–º–µ—Ä–µ–Ω–Ω—ã–π –Ω–∏—Å—Ö–æ–¥—è—â–∏–π"):
                        market_analysis += "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ–∑–∏—Ü–∏–∏. "
                    else:
                        market_analysis += "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤–æ–∑–¥–µ—Ä–∂–∞—Ç—å—Å—è –æ—Ç –≤—Ö–æ–¥–∞ –≤ —Ä—ã–Ω–æ–∫ –¥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ç–∫–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞. "
                
                if "—Ä–∏—Å–∫" in question.lower():
                    market_analysis += f"–¢–µ–∫—É—â–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {volatility:.4f} - {'–≤—ã—Å–æ–∫–∞—è' if volatility > 0.01 else '—Å—Ä–µ–¥–Ω—è—è' if volatility > 0.005 else '–Ω–∏–∑–∫–∞—è'}. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏: –Ω–µ –±–æ–ª–µ–µ 2% –æ—Ç –∫–∞–ø–∏—Ç–∞–ª–∞. "
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–∞–π–ª
            with open('visualizations/gemini_analysis.txt', 'w', encoding='utf-8') as f:
                f.write(market_analysis)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤ —Ñ–∞–π–ª
            with open('visualizations/gemini_trading_strategy.txt', 'w', encoding='utf-8') as f:
                f.write(trading_strategy)
            
            return market_analysis
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
    
    def predict_sample(self, sample_data, use_deep_learning=True):
        """
        –î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        """
        if self.model is None:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
            return None
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
        X = sample_data[self.feature_columns]
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª—å—é
        prediction = self.model.predict(X)[0]
        probabilities = self.model.predict_proba(X)[0]
        
        result = {
            'prediction': prediction,
            'probabilities': dict(zip(self.model.classes_, probabilities)),
            'model_type': 'classic'
        }
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –Ω—É–∂–Ω–æ –µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
        if self.dl_model is not None and use_deep_learning:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            sequence_length = 10  # –î–æ–ª–∂–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –¥–ª–∏–Ω–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if len(sample_data) >= sequence_length:
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ sequence_length –∑–∞–ø–∏—Å–µ–π
                sequence_data = sample_data.tail(sequence_length)[self.feature_columns].values
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
                sequence_scaled = self.scaler.transform(sequence_data)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è LSTM (–¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –±–∞—Ç—á–∞)
                sequence_reshaped = np.array([sequence_scaled])
                
                # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                dl_probabilities = self.dl_model.predict(sequence_reshaped)[0]
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
                class_indices = {'UP': 0, 'NEUTRAL': 1, 'DOWN': 2}
                dl_classes = {label: dl_probabilities[idx] for label, idx in class_indices.items()}
                
                # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
                dl_prediction = max(dl_classes.items(), key=lambda x: x[1])[0]
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                result['dl_prediction'] = dl_prediction
                result['dl_probabilities'] = dl_classes
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (—Å –≤–µ—Å–∞–º–∏)
                ensemble_probs = {}
                for cls in self.model.classes_:
                    classic_prob = result['probabilities'].get(cls, 0)
                    dl_prob = dl_classes.get(cls, 0)
                    # –í–µ—Å–∞: 0.6 –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏, 0.4 –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                    ensemble_probs[cls] = 0.6 * classic_prob + 0.4 * dl_prob
                
                # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
                ensemble_prediction = max(ensemble_probs.items(), key=lambda x: x[1])[0]
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è
                result['ensemble_prediction'] = ensemble_prediction
                result['ensemble_probabilities'] = ensemble_probs
        
        return result
    
    def create_interactive_chart(self, df, predictions=None, filename='visualizations/market_analysis.html'):
        """
        –°–æ–∑–¥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å Plotly
        """
        print("üìä –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
        fig = make_subplots(
            rows=4, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.03,
            row_heights=[0.5, 0.15, 0.15, 0.2],
            subplot_titles=('–¶–µ–Ω–∞ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã', '–û–±—ä–µ–º', 'MACD', 'RSI')
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–µ—á–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫
        fig.add_trace(
            go.Candlestick(
                x=df['timestamp'],
                open=df['open'],
                high=df['high'],
                low=df['low'],
                close=df['close'],
                name='–¶–µ–Ω–∞'
            ),
            row=1, col=1
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['sma_20'],
                name='SMA 20',
                line=dict(color='blue', width=1)
            ),
            row=1, col=1
        )
        
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['sma_50'],
                name='SMA 50',
                line=dict(color='orange', width=1)
            ),
            row=1, col=1
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º Bollinger Bands
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['bb_upper'],
                name='BB Upper',
                line=dict(color='rgba(0,128,0,0.3)', width=1)
            ),
            row=1, col=1
        )
        
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['bb_lower'],
                name='BB Lower',
                line=dict(color='rgba(0,128,0,0.3)', width=1),
                fill='tonexty'
            ),
            row=1, col=1
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä–µ–º
        fig.add_trace(
            go.Bar(
                x=df['timestamp'],
                y=df['volume'],
                name='–û–±—ä–µ–º',
                marker_color='rgba(0,0,128,0.5)'
            ),
            row=2, col=1
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º MACD
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['macd'],
                name='MACD',
                line=dict(color='blue', width=1)
            ),
            row=3, col=1
        )
        
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['macd_signal'],
                name='Signal',
                line=dict(color='red', width=1)
            ),
            row=3, col=1
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É MACD
        colors = ['green' if val >= 0 else 'red' for val in df['macd_histogram']]
        fig.add_trace(
            go.Bar(
                x=df['timestamp'],
                y=df['macd_histogram'],
                name='Histogram',
                marker_color=colors
            ),
            row=3, col=1
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º RSI
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['rsi'],
                name='RSI',
                line=dict(color='purple', width=1)
            ),
            row=4, col=1
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ –¥–ª—è RSI
        fig.add_shape(
            type="line", line_color="red", line_width=1, opacity=0.3,
            x0=df['timestamp'].iloc[0], x1=df['timestamp'].iloc[-1], y0=70, y1=70,
            xref="x4", yref="y4"
        )
        
        fig.add_shape(
            type="line", line_color="green", line_width=1, opacity=0.3,
            x0=df['timestamp'].iloc[0], x1=df['timestamp'].iloc[-1], y0=30, y1=30,
            xref="x4", yref="y4"
        )
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –Ω–∞ –≥—Ä–∞—Ñ–∏–∫
        if predictions is not None and 'direction' in predictions:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É
            last_date = df['timestamp'].iloc[-1]
            next_date = last_date + timedelta(minutes=5)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ü–µ–Ω—É
            last_price = df['close'].iloc[-1]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–µ–ª–∫–∏
            if predictions['direction'] == 'UP':
                arrow_color = 'green'
                y_end = last_price * 1.005  # +0.5%
            elif predictions['direction'] == 'DOWN':
                arrow_color = 'red'
                y_end = last_price * 0.995  # -0.5%
            else:
                arrow_color = 'gray'
                y_end = last_price
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–µ–ª–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            fig.add_annotation(
                x=next_date,
                y=last_price,
                ax=last_date,
                ay=y_end,
                xref="x",
                yref="y",
                axref="x",
                ayref="y",
                showarrow=True,
                arrowhead=3,
                arrowsize=1.5,
                arrowwidth=2,
                arrowcolor=arrow_color
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
            fig.add_annotation(
                x=next_date,
                y=y_end,
                text=f"–ü—Ä–æ–≥–Ω–æ–∑: {predictions['direction']}",
                showarrow=False,
                font=dict(color=arrow_color, size=12),
                bgcolor="white",
                bordercolor=arrow_color,
                borderwidth=1,
                borderpad=4,
                xanchor="left"
            )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–∞–∫–µ—Ç
        fig.update_layout(
            title='–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏',
            xaxis_rangeslider_visible=False,
            height=900,
            width=1200,
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        fig.write_html(filename)
        print(f"‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filename}")
        
        return fig
    
    def run_training_pipeline(self, use_deep_learning=True):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º
        """
        print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è MarketPredictor AI...")
        print("=" * 60)
        
        # 1. –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ
        df = self.fetch_market_data(days=365)
        
        # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df = self.calculate_technical_indicators(df)
        
        # 3. –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        df = self.create_target_variable(df)
        
        # 4. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        df = self.prepare_features(df)
        
        # 5. –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å
        accuracy = self.train_model(df)
        
        # 6. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = self.analyze_feature_importance(df)
        
        # 7. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è
        X = df[self.feature_columns]
        y = df[self.target_column]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        y_pred = self.model.predict(X_test)
        error_analysis = self.analyze_errors(X_test, y_test, y_pred)
        
        # 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance_history.append({
            'accuracy': accuracy,
            'date': datetime.now().isoformat(),
            'error_rate': error_analysis['error_rate'],
            'feature_importance': {feat: float(imp) for feat, imp in zip(self.feature_columns, self.model.feature_importances_)}
        })
        
        # 9. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        dl_accuracy = None
        if use_deep_learning and self.use_deep_learning:
            dl_accuracy = self.train_deep_learning_model(df)
        
        # 10. –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        self.create_interactive_chart(df)
        
        # 11. –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞
        if self.use_advanced_analysis:
            market_analysis = self.perform_advanced_analysis(df, "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞ –∏ –¥–∞–π –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ 24 —á–∞—Å–∞.")
            print("\nüß† –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞:")
            print(market_analysis)
        
        # 12. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.save_model()
        
        print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏: {accuracy:.2%}")
        if dl_accuracy:
            print(f"üß† –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {dl_accuracy:.2%}")
        print(f"üîß –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_columns)}")
        
        return df

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ MarketPredictorAI
    """
    print("\n" + "=" * 80)
    print("ü§ñ –ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û MARKET PREDICTOR AI –° –°–ê–ú–û–û–ë–£–ß–ï–ù–ò–ï–ú")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    for directory in ['models', 'data', 'visualizations']:
        os.makedirs(directory, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
    predictor = MarketPredictorAI(
        use_deep_learning=True,
        use_advanced_analysis=True
    )
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º
        df = predictor.run_training_pipeline(use_deep_learning=True)
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –∑–∞–ø–∏—Å–µ–π –∫–∞–∫ –ø—Ä–∏–º–µ—Ä –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        sample_data = df.tail(20)
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = predictor.predict_sample(sample_data)
        
        print("\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø:")
        print("-" * 50)
        print(f"üîÆ –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å: {prediction['prediction']} —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {prediction['probabilities'][prediction['prediction']]:.2%}")
        
        if 'dl_prediction' in prediction:
            print(f"üß† –ú–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {prediction['dl_prediction']} —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {prediction['dl_probabilities'][prediction['dl_prediction']]:.2%}")
        
        if 'ensemble_prediction' in prediction:
            print(f"üîÑ –ê–Ω—Å–∞–º–±–ª–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {prediction['ensemble_prediction']} —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {prediction['ensemble_probabilities'][prediction['ensemble_prediction']]:.2%}")
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
        print("\nüìä –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        visualization_data = df.tail(100)  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        prediction_for_viz = {'direction': prediction.get('ensemble_prediction', prediction['prediction'])}
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        fig = predictor.create_interactive_chart(
            visualization_data, 
            predictions=prediction_for_viz,
            filename='visualizations/final_prediction.html'
        )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞
        if predictor.use_advanced_analysis:
            print("\nüîç –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞...")
            analysis_question = "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ç—Ä–µ–Ω–¥—ã –Ω–∞ —Ä—ã–Ω–∫–µ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ 24 —á–∞—Å–∞."
            analysis = predictor.perform_advanced_analysis(visualization_data, analysis_question)
            
            print("\nüß† –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –†–´–ù–ö–ê:")
            print("-" * 50)
            print(analysis)
        
        print("\n" + "=" * 80)
        print("‚úÖ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("=" * 80)
        print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö:")
        print("üìÅ models - –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        print("üìÅ data - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        print("üìÅ visualizations - –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑")
        print("\n–û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª visualizations/final_prediction.html –≤ –±—Ä–∞—É–∑–µ—Ä–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º.")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

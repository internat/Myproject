# MarketPredictor-Cpp (EURUSD) — Full Project

Full project: C++ service with REST `/analyze` that on request collects historical OHLC data for EUR/USD (Alpha Vantage), calculates indicators, forms features, loads ONNX model (sample python pipeline for training and export included), performs prediction and returns JSON. Included — frontend (HTML/CSS/JS) with "Analyze" button and result output.

---

## Repository Structure
```
MarketPredictor-EURUSD/
├─ CMakeLists.txt
├─ README.md
├─ config/
│  └─ config.json
├─ models/
│  └─ (model_eurusd.onnx will be created by python/train_export_eurusd.py)
├─ include/
│  ├─ Config.h
│  ├─ Utils.h
│  ├─ MarketFetcher.h
│  ├─ Indicators.h
│  ├─ FeatureEngine.h
│  ├─ NewsSentiment.h
│  └─ Model.h
├─ src/
│  ├─ main.cpp
│  ├─ RestServer.cpp
│  ├─ MarketFetcher.cpp
│  ├─ Utils.cpp
│  ├─ Indicators.cpp
│  ├─ FeatureEngine.cpp
│  ├─ NewsSentiment.cpp
│  └─ Model.cpp
├─ third_party/
│  └─ (place for nlohmann/json single header or use system install)
├─ python/
│  ├─ train_export_eurusd.py
│  └─ backtest.py
├─ web/
│  ├─ index.html
│  ├─ style.css
│  └─ script.js
└─ logs/
```

---
# README.md

**Briefly:** to quickly test — run Python script `python/train_export_eurusd.py` to create `models/model_eurusd.onnx` (this is simple logistic regression), then build C++ and run. Open `web/index.html` — click "Analyze" button — frontend will send request to `http://localhost:8080/analyze` and show result.

### Dependencies
- C++17, CMake
- libcurl
- sqlite3 (опционально)
- onnxruntime (C++ API) — required to run ONNX in C++
- nlohmann/json (можно поставить как single header)
- cpp-httplib (single header, included inline in RestServer.cpp)
- Python 3.8+, numpy, pandas, scikit-learn, skl2onnx (для генерации тестовой модели)

### Quick Start (locally)
1. Install dependencies: `sudo apt install build-essential cmake libcurl4-openssl-dev libsqlite3-dev python3 python3-pip`
2. Install Python libs: `pip install numpy pandas scikit-learn skl2onnx`
3. Run: `python3 python/train_export_eurusd.py` — this will create `models/model_eurusd.onnx` (sample model).
4. Build C++:
```
mkdir build && cd build
cmake ..
make -j4
./market_predictor config/config.json
```
5. Open `web/index.html` (in browser) and click "Analyze".

---
# config/config.json

```json
{
  "market_rest_base": "https://www.alphavantage.co/query",
  "alpha_vantage_api_key": "YOUR_ALPHA_VANTAGE_KEY",
  "fx_from": "EUR",
  "fx_to": "USD",
  "interval": "1min",
  "outputsize": "compact",
  "data_source": "alpha_vantage",
  "onnx_model_path": "models/model_eurusd.onnx",
  "listen_port": 8080,
  "analyze_cooldown_seconds": 10
}
```

---
# CMakeLists.txt

```cmake
cmake_minimum_required(VERSION 3.16)
project(MarketPredictorEURUSD LANGUAGES CXX)
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

find_package(Threads REQUIRED)
find_package(CURL REQUIRED)
find_package(SQLite3 REQUIRED)

# You must provide ONNXRUNTIME_DIR or have onnxruntime installed in system
if(NOT DEFINED ENV{ONNXRUNTIME_DIR})
  message(WARNING "ONNXRUNTIME_DIR env not set. Make sure onnxruntime headers/libs are discoverable.")
endif()

include_directories(${CMAKE_SOURCE_DIR}/include)
include_directories(${CMAKE_SOURCE_DIR}/third_party)

add_executable(market_predictor
  src/main.cpp
  src/RestServer.cpp
  src/MarketFetcher.cpp
  src/Utils.cpp
  src/Indicators.cpp
  src/FeatureEngine.cpp
  src/NewsSentiment.cpp
  src/Model.cpp
)

# Link libraries - adjust path to onnxruntime if needed
find_library(ONNXRUNTIME_LIB onnxruntime PATHS $ENV{ONNXRUNTIME_DIR} /usr/local/lib /usr/lib)
if(ONNXRUNTIME_LIB)
  target_link_libraries(market_predictor PRIVATE ${ONNXRUNTIME_LIB} CURL::libcurl Threads::Threads SQLite::SQLite3)
else()
  message(WARNING "onnxruntime library not found. Build will likely fail until you install or set ONNXRUNTIME_DIR env.")
  target_link_libraries(market_predictor PRIVATE CURL::libcurl Threads::Threads SQLite::SQLite3)
endif()
```

---
# include/Config.h

```cpp
#pragma once
#include <string>
#include <nlohmann/json.hpp>

struct Config {
    std::string market_rest_base;
    std::string alpha_vantage_api_key;
    std::string fx_from;
    std::string fx_to;
    std::string interval;
    std::string outputsize;
    std::string data_source;
    std::string onnx_model_path;
    int listen_port = 8080;
    int analyze_cooldown_seconds = 10;
    static Config fromJson(const nlohmann::json &j) {
        Config c;
        c.market_rest_base = j.value("market_rest_base","https://www.alphavantage.co/query");
        c.alpha_vantage_api_key = j.value("alpha_vantage_api_key","");
        c.fx_from = j.value("fx_from","EUR");
        c.fx_to = j.value("fx_to","USD");
        c.interval = j.value("interval","1min");
        c.outputsize = j.value("outputsize","compact");
        c.data_source = j.value("data_source","alpha_vantage");
        c.onnx_model_path = j.value("onnx_model_path","models/model_eurusd.onnx");
        c.listen_port = j.value("listen_port",8080);
        c.analyze_cooldown_seconds = j.value("analyze_cooldown_seconds",10);
        return c;
    }
};
```

---
# include/Utils.h

```cpp
#pragma once
#include <string>
std::string now_iso();
```

---
# src/Utils.cpp

```cpp
#include "../include/Utils.h"
#include <chrono>
#include <ctime>
#include <iomanip>
#include <sstream>

std::string now_iso() {
    using namespace std::chrono;
    auto t = system_clock::now();
    std::time_t tt = system_clock::to_time_t(t);
    std::tm tm = *std::gmtime(&tt);
    char buf[64];
    std::strftime(buf, sizeof(buf), "%Y-%m-%dT%H:%M:%SZ", &tm);
    return std::string(buf);
}
```

---
# include/MarketFetcher.h

```cpp
#pragma once
#include <string>
#include <vector>
#include <nlohmann/json.hpp>

struct OHLC { std::string time; double open; double high; double low; double close; };

struct MarketFetcher {
    static std::string httpGet(const std::string &url);
    static std::vector<OHLC> fetchFX_OHLC_alpha(const std::string &base_url,const std::string &api_key,const std::string &from,const std::string &to,const std::string &interval,const std::string &outputsize);
};
```

---
# src/MarketFetcher.cpp

```cpp
#include "../include/MarketFetcher.h"
#include <curl/curl.h>
#include <iostream>
#include <algorithm>

using json = nlohmann::json;

static size_t write_cb(void* contents, size_t size, size_t nmemb, void* userp) {
    ((std::string*)userp)->append((char*)contents, size * nmemb);
    return size * nmemb;
}

std::string MarketFetcher::httpGet(const std::string &url) {
    CURL *curl = curl_easy_init();
    std::string readBuffer;
    if(curl) {
        curl_easy_setopt(curl, CURLOPT_URL, url.c_str());
        curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, write_cb);
        curl_easy_setopt(curl, CURLOPT_WRITEDATA, &readBuffer);
        curl_easy_setopt(curl, CURLOPT_TIMEOUT, 15L);
        CURLcode res = curl_easy_perform(curl);
        if(res != CURLE_OK) {
            std::cerr << "curl error: " << curl_easy_strerror(res) << std::endl;
        }
        curl_easy_cleanup(curl);
    }
    return readBuffer;
}

std::vector<OHLC> MarketFetcher::fetchFX_OHLC_alpha(const std::string &base_url,
                                                   const std::string &api_key,
                                                   const std::string &from,
                                                   const std::string &to,
                                                   const std::string &interval,
                                                   const std::string &outputsize) {
    std::string url = base_url + "?function=FX_INTRADAY&from_symbol=" + from + "&to_symbol=" + to +
                      "&interval=" + interval + "&outputsize=" + outputsize + "&apikey=" + api_key;
    std::string raw = httpGet(url);
    if(raw.empty()) return {};

    json j;
    try { j = json::parse(raw); } catch(const std::exception &e) { std::cerr<<"JSON parse error: "<<e.what()<<std::endl; return {}; }

    // Find time series key
    std::string ts_key;
    for (auto it = j.begin(); it != j.end(); ++it) {
        std::string k = it.key();
        if (k.find("Time Series") != std::string::npos || k.find("FX_INTRADAY") != std::string::npos || k.find("Time Series FX") != std::string::npos) { ts_key = k; break; }
    }
    if(ts_key.empty()) {
        for (auto it = j.begin(); it != j.end(); ++it) {
            if(it->is_object()) {
                for(auto &p : it.value().items()) {
                    if(p.value().is_object() && (p.value().contains("1. open") || p.value().contains("open"))) { ts_key = it.key(); break; }
                }
                if(!ts_key.empty()) break;
            }
        }
    }
    if(ts_key.empty()) { std::cerr<<"No time series found. Response snippet: "<<raw.substr(0,200)<<std::endl; return {}; }

    json ts = j[ts_key];
    std::vector<OHLC> bars;
    bars.reserve(ts.size());
    for (auto &it : ts.items()) {
        std::string time = it.key();
        json bar = it.value();
        double open=0, high=0, low=0, close=0;
        try {
            if(bar.contains("1. open")) {
                open = std::stod(bar["1. open"].get<std::string>());
                high = std::stod(bar["2. high"].get<std::string>());
                low  = std::stod(bar["3. low"].get<std::string>());
                close= std::stod(bar["4. close"].get<std::string>());
            } else if(bar.contains("open")) {
                open = bar["open"].get<double>(); high = bar["high"].get<double>(); low = bar["low"].get<double>(); close = bar["close"].get<double>();
            } else continue;
        } catch(...) { continue; }
        bars.push_back({time, open, high, low, close});
    }
    std::reverse(bars.begin(), bars.end());
    return bars;
}
```

---
# include/Indicators.h

```cpp
#pragma once
#include <vector>
using std::vector;
vector<double> sma(const vector<double>& data, int period);
vector<double> ema(const vector<double>& data, int period);
vector<double> rsi(const vector<double>& data, int period);
```

---
# src/Indicators.cpp

```cpp
#include "../include/Indicators.h"
#include <limits>
#include <vector>
using std::vector;

vector<double> sma(const vector<double>& data, int period) {
    vector<double> out(data.size(), std::numeric_limits<double>::quiet_NaN());
    if(period<=0) return out;
    double sum=0;
    for(size_t i=0;i<data.size();++i){ sum += data[i]; if(i>= (size_t)period) sum -= data[i-period]; if(i+1>= (size_t)period) out[i]=sum/period; }
    return out;
}

vector<double> ema(const vector<double>& data, int period) {
    vector<double> out(data.size(), std::numeric_limits<double>::quiet_NaN());
    if(data.empty()||period<=0) return out;
    double k = 2.0/(period+1);
    double prev = data[0]; out[0]=prev;
    for(size_t i=1;i<data.size();++i){ prev = data[i]*k + prev*(1-k); out[i]=prev; }
    return out;
}

vector<double> rsi(const vector<double>& data, int period){
    vector<double> out(data.size(), std::numeric_limits<double>::quiet_NaN());
    if(data.size()<= (size_t)period) return out;
    double gain=0, loss=0;
    for(size_t i=1;i<= (size_t)period; ++i){ double diff=data[i]-data[i-1]; if(diff>0) gain+=diff; else loss -= diff; }
    gain/=period; loss/=period; out[period]=100 - (100/(1+gain/(loss+1e-12)));
    for(size_t i=period+1;i<data.size();++i){ double diff=data[i]-data[i-1]; if(diff>0){ gain=(gain*(period-1)+diff)/period; loss=(loss*(period-1))/period; } else { loss=(loss*(period-1)-diff)/period; gain=(gain*(period-1))/period; } out[i]=100 - (100/(1+gain/(loss+1e-12))); }
    return out;
}
```

---
# include/FeatureEngine.h

```cpp
#pragma once
#include <vector>
struct FeatureEngine { static std::vector<double> buildFeatures(const std::vector<double>& closes); };
```

---
# src/FeatureEngine.cpp

```cpp
#include "../include/FeatureEngine.h"
#include "../include/Indicators.h"
#include <vector>
using std::vector;

std::vector<double> FeatureEngine::buildFeatures(const std::vector<double>& closes) {
    std::vector<double> feats;
    if(closes.size() < 50) return feats;
    auto sma20 = sma(closes, 20);
    auto ema12 = ema(closes, 12);
    auto rsi14 = rsi(closes, 14);
    size_t i = closes.size()-1;
    feats.push_back(closes[i]);
    feats.push_back(sma20[i]);
    feats.push_back(ema12[i]);
    feats.push_back(rsi14[i]);
    feats.push_back(closes[i] - sma20[i]);
    feats.push_back(closes[i] - ema12[i]);
    return feats;
}
```

---
# include/NewsSentiment.h

```cpp
#pragma once
#include <string>
struct NewsSentiment { static int scoreText(const std::string &text); };
```

---
# src/NewsSentiment.cpp

```cpp
#include "../include/NewsSentiment.h"
#include <algorithm>

int NewsSentiment::scoreText(const std::string &text) {
    static std::vector<std::string> pos = {"gain","rise","bull","positive","beats","surge","up"};
    static std::vector<std::string> neg = {"drop","falls","bear","negative","miss","plunge","down"};
    std::string t = text; std::transform(t.begin(), t.end(), t.begin(), ::tolower);
    int sc=0; for(auto &p: pos) if(t.find(p)!=std::string::npos) sc+=1; for(auto &n: neg) if(t.find(n)!=std::string::npos) sc-=1; return sc;
}
```

---
# include/Model.h

```cpp
#pragma once
#include <memory>
#include <vector>
#include <string>
#include <onnxruntime_cxx_api.h>

struct Model {
    Ort::Env env;
    std::unique_ptr<Ort::Session> session;
    Model(const std::string &path);
    std::vector<float> predict(const std::vector<double> &features);
};
```

---
# src/Model.cpp

```cpp
#include "../include/Model.h"
#include <iostream>

Model::Model(const std::string &path) : env(ORT_LOGGING_LEVEL_WARNING, "mp") {
    Ort::SessionOptions opts; opts.SetIntraOpNumThreads(1);
    try{ session = std::make_unique<Ort::Session>(env, path.c_str(), opts); }
    catch(const std::exception &e){ std::cerr<<"ONNX load error: "<<e.what()<<std::endl; throw; }
}

std::vector<float> Model::predict(const std::vector<double> &features) {
    std::vector<float> input; input.reserve(features.size()); for(auto &v: features) input.push_back((float)v);
    std::vector<int64_t> dims = {(int64_t)1, (int64_t)input.size()};
    size_t input_tensor_size = input.size();
    Ort::MemoryInfo mem_info = Ort::MemoryInfo::CreateCpu(OrtAllocatorType::OrtArenaAllocator, OrtMemTypeDefault);
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(mem_info, input.data(), input_tensor_size, dims.data(), dims.size());
    auto allocator = Ort::AllocatorWithDefaultOptions();
    const char* input_name = session->GetInputNameAllocated(0, allocator).get();
    const char* output_name = session->GetOutputNameAllocated(0, allocator).get();
    auto output_tensors = session->Run(Ort::RunOptions{nullptr}, &input_name, &input_tensor, 1, &output_name, 1);
    float* floatarr = output_tensors.front().GetTensorMutableData<float>();
    size_t out_size = output_tensors.front().GetTensorTypeAndShapeInfo().GetElementCount();
    std::vector<float> out(floatarr, floatarr + out_size);
    return out;
}
```

---
# src/RestServer.cpp

```cpp
// Simple REST server using cpp-httplib (single header). We embed the minimal header implementation here.
#include "../include/Config.h"
#include "../include/MarketFetcher.h"
#include "../include/FeatureEngine.h"
#include "../include/Model.h"
#include "../include/NewsSentiment.h"
#include "../include/Utils.h"
#include <thread>
#include <atomic>
#include <fstream>
#include <iostream>

// Minimal httplib inclusion (assumes httplib.h is in third_party or system include)
#include "../third_party/httplib.h"

using json = nlohmann::json;

struct RestServer {
    Config cfg;
    std::atomic<bool> is_running{false};
    std::chrono::system_clock::time_point last_run = std::chrono::system_clock::now() - std::chrono::hours(1);
    RestServer(const Config &c): cfg(c) {}
    void start(int port) {
        httplib::Server svr;
        svr.Post("/analyze", [&](const httplib::Request &req, httplib::Response &res){
            auto now = std::chrono::system_clock::now();
            auto secs = std::chrono::duration_cast<std::chrono::seconds>(now - last_run).count();
            if(secs < cfg.analyze_cooldown_seconds) {
                res.status = 429; res.set_content("{\"error\":\"Cooldown\"}", "application/json"); return;
            }
            if(is_running.exchange(true)) { res.status = 423; res.set_content("{\"error\":\"Already running\"}", "application/json"); return; }
            last_run = now;
            // Read body optional params
            json body;
            try { if(!req.body.empty()) body = json::parse(req.body); } catch(...){}

            // Run single analysis in same thread (synchronous)
            json reply;
            try {
                // fetch data
                auto bars = MarketFetcher::fetchFX_OHLC_alpha(cfg.market_rest_base, cfg.alpha_vantage_api_key, cfg.fx_from, cfg.fx_to, cfg.interval, cfg.outputsize);
                if(bars.size() < 60) { reply["error"] = "Not enough bars"; res.set_content(reply.dump(), "application/json"); is_running=false; return; }
                std::vector<double> closes; for(auto &b: bars) closes.push_back(b.close);
                auto feats = FeatureEngine::buildFeatures(closes);
                if(feats.empty()) { reply["error"]="Failed to build features"; res.set_content(reply.dump(), "application/json"); is_running=false; return; }
                // optionally compute news score (omitted network calls here)
                feats.push_back(0.0);
                // load model
                Model model(cfg.onnx_model_path);
                auto out = model.predict(feats);
                // find argmax
                size_t idx=0; float maxv=-1e9; for(size_t i=0;i<out.size();++i) if(out[i]>maxv){maxv=out[i]; idx=i;}
                std::string label = (idx==2?"UP":(idx==0?"DOWN":"NEUTRAL"));
                reply["symbol"] = cfg.fx_from + "/" + cfg.fx_to;
                reply["timestamp"] = now_iso();
                reply["prediction"] = label;
                reply["confidence"] = maxv;
                reply["meta"] = { {"model","eurusd_sample"}, {"features_count", (int)feats.size()}, {"data_source", cfg.data_source} };
                res.set_content(reply.dump(), "application/json");
            } catch(const std::exception &e) {
                reply["error"] = e.what(); res.set_content(reply.dump(), "application/json");
            }
            is_running=false;
        });

        svr.Get("/ping", [](const httplib::Request&, httplib::Response &res){ res.set_content("pong", "text/plain"); });
        std::cout<<"Starting server on port "<<port<<std::endl;
        svr.listen("0.0.0.0", port);
    }
};
```

---
# src/main.cpp

```cpp
#include <iostream>
#include <fstream>
#include <nlohmann/json.hpp>
#include "../include/Config.h"
#include "../include/Utils.h"
#include "../include/MarketFetcher.h"
#include "../include/FeatureEngine.h"
#include "../include/Model.h"
#include "../include/NewsSentiment.h"
#include "../src/RestServer.cpp" // simple include for single-file run

using json = nlohmann::json;
int main(int argc, char** argv) {
    std::string config_path = "config/config.json";
    if(argc>1) config_path = argv[1];
    std::ifstream f(config_path);
    if(!f) { std::cerr<<"Cannot open config: "<<config_path<<std::endl; return 1; }
    json j; f>>j;
    Config cfg = Config::fromJson(j);
    RestServer server(cfg);
    server.start(cfg.listen_port);
    return 0;
}
```

---
# python/train_export_eurusd.py

```python
# Простой скрипт: генерируем синтетические данные и обучаем multinomial LogisticRegression, экспортируем в ONNX
import numpy as np
import sklearn
from sklearn.linear_model import LogisticRegression
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType
import os

os.makedirs('models', exist_ok=True)
# Генерация синт. данных: X shape (1000,6)
X = np.random.randn(1000,6).astype(np.float32)
# Метки: 0=DOWN,1=NEUTRAL,2=UP
y = np.random.randint(0,3, size=(1000,))
clf = LogisticRegression(multi_class='multinomial', max_iter=200)
clf.fit(X,y)
initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]
onnx_model = convert_sklearn(clf, initial_types=initial_type)
with open('models/model_eurusd.onnx','wb') as f:
    f.write(onnx_model.SerializeToString())
print('Saved models/model_eurusd.onnx')
```

---
# python/backtest.py

```python
# Very simple backtest that loads features from CSV and model predictions (not implemented fully) — placeholder
print('Backtest placeholder - extend with historical data and strategy rules')
```

---
# third_party/httplib.h

```cpp
// For brevity: include the single header httplib.h (detailed header not embedded here). Place the file from https://github.com/yhirose/cpp-httplib/blob/master/httplib.h into third_party/httplib.h
```

---
# web/index.html

```html
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MarketPredictor EUR/USD</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="card">
    <h1>MarketPredictor — EUR/USD</h1>
    <p>Click button to perform one-time analysis (Alpha Vantage)</p>
    <button id="analyze">Analyze</button>
    <div id="status"></div>
    <pre id="result"></pre>
  </div>
  <script src="script.js"></script>
</body>
</html>
```

---
# web/style.css

```css
body{font-family:Inter,Arial,Helvetica,sans-serif;background:#f3f4f6;display:flex;align-items:center;justify-content:center;height:100vh;margin:0}
.card{background:#fff;padding:24px;border-radius:8px;box-shadow:0 6px 18px rgba(0,0,0,0.08);width:420px}
button{background:#2563eb;color:#fff;padding:10px 16px;border:none;border-radius:6px;cursor:pointer}
button:disabled{opacity:0.6}
#status{margin-top:12px;color:#374151}
#result{background:#f9fafb;padding:12px;border-radius:6px;margin-top:12px;overflow:auto}
```

---
# web/script.js

```javascript
const analyzeBtn = document.getElementById('analyze');
const statusDiv = document.getElementById('status');
const resultPre = document.getElementById('result');

async function analyze(){
  analyzeBtn.disabled = true; statusDiv.textContent = 'Request sent...'; resultPre.textContent='';
  try{
    const resp = await fetch('http://localhost:8080/analyze',{method:'POST',headers:{'Content-Type':'application/json'}, body: JSON.stringify({})});
    if(resp.status!==200){ const text = await resp.text(); statusDiv.textContent = 'Error: '+resp.status; resultPre.textContent = text; analyzeBtn.disabled=false; return; }
    const data = await resp.json(); statusDiv.textContent = 'Done: '+(data.prediction||''); resultPre.textContent = JSON.stringify(data,null,2);
  }catch(e){ statusDiv.textContent = 'Network error: '+e.message; }
  analyzeBtn.disabled=false;
}

analyzeBtn.addEventListener('click', analyze);
```

---

## What I CANNOT do automatically here
- I cannot upload binary onnxruntime or onnx binary to document. But you can generate `models/model_eurusd.onnx` locally by running `python/train_export_eurusd.py`.
- I cannot execute `cmake`/`make` on your system — need to build locally.

## Next Steps
If everything is satisfactory, download/clone this structure, place `third_party/httplib.h` (from cpp-httplib repo) and `third_party/json.hpp` (nlohmann), install onnxruntime (or set `ONNXRUNTIME_DIR` environment variable) and build. If you want, I can provide more detailed installation instructions for onnxruntime on Ubuntu/Windows.

---

If needed — I can make adjustments (e.g., add TwelveData adapter, improve features, connect economic calendars, or prepare Dockerfile). Write "ready, building" if you want me to generate additional instructions for building onnxruntime on Linux or Windows.
